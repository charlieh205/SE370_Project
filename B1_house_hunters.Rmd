---
title: "House Hunters"
author: "Charlie Harrington and Karlee Scott"
date: "5/11/2021"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(tidyverse)
```
# Introduction

### Problem Statement and Background
Karlee Scott selected Hunter Army Air Field as her next duty station. Her boyfriend selected Fort Stewart. In this project we analyzed the value and price of houses in the Savannah, Georgia area to offer house recommendations that provide value for a reasonable price. Our problem statement was: Which houses in Savannah, Georgia should Karlee Scott consider based on what is valuable to her and if that value is selling for a reasonable price?

### Bottom Line Up Front

# Methodology

### Webscrape from Zillow
We first opened a web browser and navigated to Zillow. Once on Zillow, we filtered the results to houses under $250,000 with air conditioning in the Savannah area. The URL contained all of these filters, so no matter what computer we used to navigate to that URL the same filters would be applied. Commute destinations to Hunter Army Air Field and Fort Stewart were not contained in the URL thus we had to use a package called RSelenium which created a server that we could type in the commute destinations manually and then we could read from that source page. We used the R function read.html from the package rvest to begin extracting data from the website. Because our end goal was to extract data from every house, we needed the URL for every house. Zillow only showed 40 houses per page of results, therefore we had to first isolate the URL for each page of results and then compile the 40 URLs for every house on each of those result pages to obtain the specific URL for each house. We used a for loop and navigated to each house's URL grabbing the number of bedrooms, bathrooms, square feet, year built, acres, commute times, HOA fees, and price each time. The issue with a for loop was that by scraping data systematically we caused a pop up from Zillow asking to verify that we were not a robot which would end the data collection. To fix this we added a sleep function inside the for loop which would pause between grabs for a random amount of time to prevent Zillow from suspecting we were a robot. This gave us all the necessary data we needed from Zillow.

### Clean Data
Next we cleaned the data. This started by first inspecting our data frame. We cleaned each column one at a time. All of the variables were characters and we needed them to be numerics to eventually use in our value functions. Most of the data contained characters that did not easily translate to numerics for example for HOA fee some said "HOA: no" which numerically means zero. Another challenge was with commute times, some were in minutes and others were in hours so we had to make sure they were all in the same units. If the data pulled was incorrect for a column we replaced it with an NA. NAs neither added nor detracted from the total value of the house. The only homes we removed entirely were ones with no list price or if there was no specific address which would prevent us from plotting the location (most of these occurred with buildable plans).

### Find House Value
With the cleaned data which had numeric values for all variables: bedrooms, bathrooms, square feet, year built, acres, commute times, and HOA fees we were able to find the individual value for each aspect of interest provided. First, we had to create value functions based on what Karlee's input for each aspect of interest. Most value functions were s-curves for example from zero square feet to about 1,500 square feet there is increasing returns but once the square footage exceeds 1,500 square feet it was diminishing returns. We included a plot for the square feet value function below.

```{r, include = FALSE}
sCurve <- function(x, location=0, scale=1, shape=1) {
  z <- scale*(x - location)
  retval <- z/(1 + abs(z)^shape)^(1/shape)
  #make values between 0 and 1 instead of -1 and 1
  retval <- (retval/2)+.5
  return(retval)
}

x <- c(1000,1100,1200,1300,1400,1500,1600,1700,1800,1900,2000)
y <- sCurve(x,location = 1500, scale = 1/100)
plot_data <- data.frame(cbind(x,y))

```
```{r,echo=FALSE,fig.align = 'center'}
ggplot(plot_data,aes(x,y))+geom_point()+ggtitle("Square Feet versus Value")+xlab("square feet")+ylab("value")
```

After we found the value of each aspect of interest for each house we determined the total value of each house weighing each aspect of interest according to their relative importance provided by Karlee. We used the following equation to calculate the total value of ahouse by summing each aspect of interest's value multiplied by its respective weight and then dividing that sum by the sum of the weights.
$$
\frac{3(bed)+2(bath)+2(square\_feet)+4(hunter\_drive\_time)+3(stewart\_drive\_time)+acres+3(HOA\_fee)}{18}
$$

### Bin Value and Price into High, Medium, and Low
We ultimately wanted to find houses that offered high value for a low cost. To make this easier we binned value and price into three categories: high, medium, and low. Instead of setting specific values or prices for each bin based on what we thought contintued as a high, medium, and low value or price we used percentiles instead. Low price was between \$0 and \$151,900, medium price was between \$151,900 and \$200,000%, and high value was between \$200,000 and \$250,000% (no house price exceeded $250,000). Low value was between 0 and 44%, medium value was between 44% and 55%, and high value was between 55% and 84% (no house value exceeded 84%). We show these percentiles below.
```{r, include = FALSE}
df <- read.csv("C:/Users/Karlee Scott/OneDrive - West Point/AY 21-2/SE370 Computer Aided Design/B1_HouseHunters_folder/value_house.csv")
df <- df[,-1]
price_bins <- quantile(df[,"list_price"], c(.33, .66, 1), na.rm = TRUE) 
value_bins <- quantile(df[,"value"], c(.33, .66, 1), na.rm = TRUE)  
```
```{r}
price_bins
value_bins
```

### Add Latitude and Longitude using RSelenium
We wanted to display the houses on a map. To do so we need the latitude and longitude location of every house. Most R packages that over address to latitude and longitude conversions require access to a Google API which is very expensive. Instead we used the website https://www.latlong.net/ which takes an address as an input and outputs the corresponding latitude and longitude. This website required payment over five grabs. 250 grabs costed \$3.84. We needed to conduct 210 grabs for the 210 houses of interest. We paid the \$3.84 which also removed pop-up adds which would interupt our data collection. The image below shows what this website looks like.
<center>
![](C:/Users/Karlee Scott/OneDrive - West Point/AY 21-2/SE370 Computer Aided Design/B1_HouseHunters_folder/latlon.png)
</center>
We discussed RSelenium previously which allowed us to manually change aspects of our server and scrape from the website with those additional inputs. Although we could have manually typed each address into the browser and scraped the latitude and longitude, this would take a while. Instead, RSelenium allows users to use R code to interact with a website rather than a user manually iteracting with the website. In our case, we had RSelenium type in the address for a house in the Place Name box, click the Find button, clear the Place Name box and repeat for all 210 houses while webscraping the latitude and longitude for each repetition and adding them to our data frame. We included a snippet of the R code that performed these actions below.
<center>
![](C:/Users/Karlee Scott/OneDrive - West Point/AY 21-2/SE370 Computer Aided Design/B1_HouseHunters_folder/RSelenium_complete.png)
</center>

### Final Data
We included the first five rows of the final data in the table below. This dataframe contains all of the cleaned data, the added latitude and longitudes, the total value for each house, the value and price bins, as well as a column with value per dollar.
```{r}
head(df)
```
# Results and Analysis

# Conclusion and Recommendations
